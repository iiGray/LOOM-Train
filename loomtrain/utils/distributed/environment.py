import os, time, pdb, functools
from typing import Optional, Union, List, Literal
import multiprocessing as mp
from functools import partial
from contextlib import contextmanager
import sys
from enum import Enum, auto


_CURRENT_GROUP_KEY_ = "_CURRENT_GROUP_"

class _Marks(Enum):
    start = auto()
    sep = auto()
    end = auto()
    recover = auto()
class Marks(Enum):
    ckpt = auto()
    stop = auto()


def _assert_init(func):
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        assert Environment._manager is not None, \
            "You didn't initialize the environment, please add : 'mpenv.set_start_method()'."
        return func(*args, **kwargs)
    return wrapper


class _Group:
    @_assert_init
    def __init__(self, world_size: int):
        
        self.world_size = world_size

        self._comm_list = Environment._manager.list(
            [None for _ in range(world_size * world_size)]
        )

        self.barrier_lock = Environment._manager.Lock()
        self.barrier_count = Environment._manager.Value("i", 0)
        self.barrier_condition = Environment._manager.Condition(self.barrier_lock) 
        self.rank_queue = mp.Queue()
        self.registered_processes = Environment._manager.list()
        
        for rank in range(world_size): self.rank_queue.put(rank)

        self.rank_map = Environment._manager.dict()
        
        Environment._current_group = self
        Environment._group_count += 1
        Environment._process_count += world_size

    def barrier(self):
        with self.barrier_condition:
            self.barrier_count.value += 1 
            if self.barrier_count.value < self.world_size:
                self.barrier_condition.wait()
            else:
                self.barrier_count.value = 0
                self.barrier_condition.notify_all()



class Environment:
    """
    This class is used to create processes group and allocate rank for each process in a group.
    Allow direct use, as shown in the example. 

    Examples:
        >>> @mpenv.need_rank
        >>> def worker1(cls):
        >>>     print("outter: ", mpenv.get_rank(), mpenv.get_world_size(), mpenv.getpid())
        >>>
        >>> @mpenv.need_rank
        >>> def worker2(cls):
        >>>     print("inner: ", mpenv.get_rank(), mpenv.get_world_size(), mpenv.getpid())
        >>>
        >>> processes1,processes2 = [], []
        >>> for i in range(4):
        >>>     process = mp.Process(target = A.worker1, args=())
        >>>     processes1 += [process]
        >>> with mpenv.add_process_group(world_size = 4):
        >>>     for p in  processes1:
        >>>         p.start()
        >>> for p in processes1:
        >>>     p.join()
        >>> 
        >>> with mpenv.add_process_group(world_size = 3):
        >>>     for i in range(3):
        >>>         process = mp.Process(target = A.worker2,args=())
        >>>         process.start()
        >>> 
        >>>         processes2 += [process]    
        >>>     for p in processes2:
        >>>         p.join()
    """
    _indent: int = 0
    _group_count: int = 0
    _process_count: int = 0
    _current_group: _Group = None

    _manager = None
    

    @classmethod
    def set_start_method(cls, method: str = "fork", force: bool = False):
        mp.set_start_method(method = method, force = force)
        cls._init_manager()
    
    @classmethod
    def get_start_method(cls, allow_none: bool = False) -> str:
        return mp.get_start_method(allow_none = allow_none)

    @classmethod
    def _init_manager(cls):
        if cls._manager is None:
            cls._manager = mp.Manager()

    @classmethod
    def _active_pid(cls):
        return set(p.pid for p in mp.active_children())

    @classmethod
    def _init_group(cls, world_size):
        return _Group(world_size = world_size)

    @classmethod
    @contextmanager
    def add_process_group(cls, world_size):

        if cls._indent:
            yield
            raise RuntimeError("Nested calling `with add_process_group(wolrd_size)` is not allowed!")
        
        cls._indent += 1
        
        group = cls._init_group(world_size = world_size)
        enter_pids = cls._active_pid()

        yield

        exit_pids  = cls._active_pid()
        this_pids = set(group.registered_processes) | (exit_pids - enter_pids)
        

        if len(this_pids) < group.world_size:
            raise ValueError("You set a world_size larger than the total number of "
                        "sub-processes that join the process group.")
            
        start_wait = time.time()
        while group.rank_queue.qsize(): # waiting all ranks to be allocated
            if time.time() - start_wait > group.world_size  * 10:
                raise ValueError("You set a world_size larger than the total number of "
                            "sub-processes that join the process group.")
            
        cls._indent -= 1
        cls._current_group = None

    @classmethod
    def start_process_group(cls, processes: List[mp.Process | dict]):
        if cls._indent:
            raise RuntimeError("Calling `start_process_group(processes)` in "
                               "`with add_process_group(world_size)` is not allowed!")
        
        group = cls._init_group(world_size = len(processes))
        enter_pids = cls._active_pid()

        for process in processes: 
            if isinstance(process, dict): process = mp.Process(**process)
            process.start()

        exit_pids = cls._active_pid()
        this_pids = set(group.registered_processes) | (exit_pids - enter_pids)

        if len(this_pids) < group.world_size:
            raise ValueError("You set a world_size larger than the total number of "
                        "sub-processes that join the process group.")

        start_wait = time.time()
        while group.rank_queue.qsize(): # waiting all ranks to be allocated
            if time.time() - start_wait > group.world_size/10:
                raise ValueError("You set a world_size larger than the total number of "
                            "sub-processes that join the process group.")
    
    @classmethod
    def need_rank(cls, func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            pid = os.getpid()
            cls._current_group = kwargs.pop(
                _CURRENT_GROUP_KEY_, cls._current_group
            )
            if cls._current_group is None:
                raise RuntimeError("You aimed to allocate rank for each process, but didn't "
                                   "run `with Environment.add_process_group(world_size)`.")
            group = cls._current_group
            group.registered_processes.append(pid)

            if group.rank_queue.qsize() == 0:
                raise ValueError("You set a world_size less than the total number of "
                                 "sub-processes that join the process group.")
            
            group.rank_map[pid] = group.rank_queue.get()
            
            ret = func(*args, **kwargs)

            return ret
        return wrapper
    
    @classmethod
    def wrap(cls, target):
        if cls.get_start_method() == "spawn":
            target = partial(target, **{_CURRENT_GROUP_KEY_ : cls._current_group})
        return target

    @classmethod
    def Process(cls, target, args):
        target = cls.wrap(target)
        process = mp.Process(target = target, 
                             args = args)

        return process
    

    @classmethod
    @_assert_init
    def set_pdb_trace(cls, rank: int = None):
        if rank !=None and cls.get_rank()!=rank: 
            return
        sys.stdin = open(0)  
        pdb.set_trace()
    
    
    @classmethod
    def getpid(cls):
        return os.getpid()
    
    @classmethod
    def getppid(cls):
        return os.getppid()
    
    @classmethod
    def getgid(cls):
        return cls._group_count - 1
    
    @classmethod
    def get_world_size(cls):
        '''run in child processes'''
        return cls._current_group.world_size

    @classmethod
    def get_rank(cls):
        '''run in child processes'''
        return cls._current_group.rank_map[os.getpid()]
    
    @classmethod
    def get_global_rank(cls):
        return cls._process_count - cls._current_group.world_size + cls.get_rank()
    
    @classmethod
    def barrier(cls):
        cls._current_group.barrier()
    

    
    @classmethod
    def gather(cls, obj, dst: int = 0):
        cls._current_group._comm_list[cls.get_rank()] = obj
        cls.barrier()
        gathered = []
        if dst == cls.get_rank():
            gathered.extend(list(cls._current_group._comm_list[: cls.get_world_size()]))
            del cls._current_group._comm_list[-cls.get_world_size(): ]
            cls._current_group._comm_list.extend([None] * cls.get_world_size())
        return gathered
    
    @classmethod
    def all_gather(cls, obj):
        cls._current_group._comm_list[cls.get_rank()] = obj
        cls.barrier()
        gathered = list(cls._current_group._comm_list[:  cls.get_world_size()])
        cls.barrier()
        del cls._current_group._comm_list[-cls.get_rank():]
        cls._current_group._comm_list.extend([None] * cls.get_world_size())
        cls.barrier()
        return gathered

    @classmethod
    def set_cuda_visible_devices(cls, devices: str):
        os.environ["CUDA_VISIBLE_DEVICES"] = devices

    @classmethod
    def get_cuda_visible_devices(cls):
        return os.environ["CUDA_VISIBLE_DEVICES"]
    
    @classmethod
    def set_cuda_alloc_conf(cls, expandable_segments: bool = True):
        os.environ["PYTORCH_CUDA_ALLOC_CONF"] = f"expandable_segments:{expandable_segments}"

    @classmethod
    def set_hf_endpoint(cls, hub: Literal["huggingface",
                                          "hf-mirror"] = "hf-mirror"):
        
        url = f"https://{hub}.co{'m' if 'hf' in hub else 'm'}"
        os.environ["HF_ENDPOINT"] = url
    @classmethod
    def set_tokenizers_parallelism(cls, enable: bool = True):
        os.environ["TOKENIZERS_PARALLELISM"] = str(enable).lower()

    @classmethod
    def set_triton_debug(cls, enable: bool = True):
        os.environ["TRITON_DEBUG"] = str(int(enable))

    @classmethod
    def set_cuda_launch_blocking(cls, enable: bool = True):
        os.environ["CUDA_LAUNCH_BLOCKING"] = str(int(enable))

    @classmethod
    def set_torch_use_cuda_dsa(cls, enable: bool = True):
        os.environ["TORCH_USE_CUDA_DSA"] = str(int(enable))

    @classmethod
    def set_vllm_enable_v1_multiprocessing(cls, enable: bool = False):
        os.environ["VLLM_ENABLE_V1_MULTIPROCESSING"] = str(int(enable))

    @classmethod
    def get_cuda_devices_count(cls):
        import pynvml
        pynvml.nvmlInit()
        count = pynvml.nvmlDeviceGetCount()
        pynvml.nvmlShutdown()
        return count
    
    @classmethod
    def get_cuda_memory_info(cls, device: int = 0):
        '''return total, used, free'''
        import pynvml
        pynvml.nvmlInit()
        handle = pynvml.nvmlDeviceGetHandleByIndex(device)  # 0 表示第 0 块 GPU
        info = pynvml.nvmlDeviceGetMemoryInfo(handle)
        total, used, free = \
            info.total//1024**2, info.used//1024**2, info.free//1024**2
        pynvml.nvmlShutdown()

        return dict(
            total = total,
            used = used,
            free = free,
            used_ratio =  used/total,
        )

    @classmethod
    def get_idle_gpus(cls):
        return [
            i for i in range(cls.get_cuda_devices_count()) \
            if cls.get_cuda_memory_info(i)["used_ratio"] < 1/100
        ]

    @classmethod
    def wait_for_idle_gpus(cls, count: int = 8, timeout: int = float("inf")):
        start = time.time()
        while True:
            idle_gpu_lists = cls.get_idle_gpus()
            if len(idle_gpu_lists) >= count:
                return idle_gpu_lists[: count]
            time.sleep(5)
            if time.time() - start > timeout:
                raise RuntimeError("timeout")